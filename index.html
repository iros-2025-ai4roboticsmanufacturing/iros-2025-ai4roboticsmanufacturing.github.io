<!DOCTYPE html>
<html lang="en">
<link
  rel="stylesheet"
  href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css"
/>
<head>
  <link rel="preconnect" href="https://fonts.gstatic.com">
  <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;600&display=swap" rel="stylesheet">
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="">
  <meta name="author" content="">
  <title>IROS 2025 - Genenrative AI for Robotics and Smart Manufacturing</title>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.16.0/umd/popper.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>
  <!-- Latest compiled and minified JavaScript -->
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"
    integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa"
    crossorigin="anonymous"></script>
  <link rel="stylesheet" href="http://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
  <!-- Custom styles for this template -->
  <link href="./css/scrolling-nav.css" rel="stylesheet">
  <link href="./css/style.css" rel="author stylesheet">
  <style>
    .pc-column {
      width: 270px;
      display: inline-block;
      vertical-align: top;
    }

    .pc_list_item {
      display: inline-block;
      width: 200px;
    }
  </style>
</head>

<body id="page-top">
  <!-- Navigation -->
  <nav class="navbar navbar-expand-lg navbar-light bg-light" id="mainNav">
    <div class="container bar-container">
      <a class="title-head" href="#page-top">GenAI4RM</a>
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarResponsive"
        aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarResponsive">
        <ul class="navbar-nav ml-auto">
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#about">About</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#speakers">Speakers</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#schedule">Schedule</a>
          </li>
          <!--li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#callpapers">Call for Papers </a>
            </li-->
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#papers">Accepted Papers</a>
          </li>
          <!-- <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#faq">FAQ</a>
              </li> -->
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#organizers">Organizers</a>
          </li>
        </ul>
      </div>
    </div>
  </nav>
  <header class="headercontainer bg-primary text-white" style="padding: 0%; max-height: none; ">
    <div style="background-color: rgba(160,160,160,0.0)" class="text-center">
      <div
        style="padding-bottom: 6%; padding-top: 6%; background-image: url('./images/background.png'); background-size: cover; background-position: center">
        <div class="container titlebox" ; style="display: inline-block; background-color:rgba(0,0,0, 0.7); width:auto;">
          <p style="text-align: center; margin-bottom: 2" class="subtitle">IEEE/RSJ International Conference on
            Intelligent Robots and Systems (IROS 2025) </p>
          <p style="text-align: center; margin-bottom: 2" class="title">Workshop on <b></b>Generative AI for Robotics
            and Smart Manufacturing</b></p>
          <p style="text-align: center; margin-bottom: 0" class="subtitle"> 20 - 24 October, 2025 | Hangzhou, China</p>
          <!-- <p style="text-align: center; margin-bottom: 0" class="subtitle">Room: Orion (floor 2), 8:30 am - 12:30 pm | November 9 | Munich, Germany</p> -->
        </div>
      </div>
    </div>
  </header>
  <hr class="half-rule" />
  <section id="about" style="padding:70px 0 0 0">
    <div class="container">
      <div class="row">
        <div class="col-md-10 mx-auto">
          <!-- div style="justify-content: center; text-align:center;"><img src="https://semrob.github.io/images/semrob_2024_logo.png" width="700px"></div-->
          <span class=titlesec>Abstract</span>
          <br>
          <span>
            The rapid advancement of generative artificial intelligence (AI) and foundational models, such as large
            language models (LLMs), is poised to transform the fields of robotics and manufacturing. These advanced AI
            technologies offer unprecedented capabilities in robot skills, control, interactions, and the development of
            general-purpose manipulation skills. Simultaneously, they are driving significant advancements in
            manufacturing systems, industrial digitalization, and automation. However, the deployment of AI-enabled
            robots at an industrial scale to advance manufacturing remains an open challenge. This workshop seeks to
            explore the cutting-edge applications of generative AI in these domains.

            In robotics, the focus will be on the application of generative AI and foundational models to enable a
            generalist robot. AI is accelerating the development of robotic skills, including general-purpose
            manipulation, allowing robots to perform a wider range of tasks in diverse environments. Generative AI and
            foundational models are being leveraged to optimize robot-assisted manufacturing systems, drive industrial
            digitalization, and enable advanced automation. These technologies are at the core of smart manufacturing,
            where AI-driven decision-making processes lead to more efficient, flexible, and resilient production
            systems.

            This workshop is to encourage state-of-the-art and visionary research works on generative AI for robotics
            and manufacturing. Topics will include but not limited to: AI-powered industrial robotics and automation,
            foundation models in robotics and manufacturing, and AI-accelerated machine intelligence.

            Attendees will have opportunities to engage in face-to-face discussions with renowned researchers, gain
            hands-on experiences with the latest AI technologies in robotics and manufacturing, and network with
            researchers from the United States, Europe, and Asia.
          </span>
        </div>
      </div>
    </div>
  </section>
  <section id="event">
    <div class="container">
      <div class="row">
        <div class="col-md-10 mx-auto">
          <span class=titlesec>Event Information</span>
          <span>
            <!-- This is a primarily in-person workshop, held at the <i>2025 IEEE/RSJ International Conference on Intelligent Robots and Systems</i> (IROS), in Hangzhou, China on <b>20-24 October 2025</b>, starting at <b>08:30 CET</b>. -->
            This is a primarily in-person workshop, held at the <i>2025 IEEE/RSJ International Conference on Intelligent
              Robots and Systems</i> (IROS), in Hangzhou, China on 20-24 October 2025.
              
           <div style="margin-top: 6px;"></div>
          <i class="fa-solid fa-calendar-days" style="margin-right: 6px;"></i>  <b>Oct. 24, 2025, Full-day</b>.

          <div style="margin-top: 6px;"></div>
          <i class="fa-solid fa-location-dot" style="margin-right: 6px;"></i> <b>Room: 103B</b>, <b>Session 22</b>, Hangzhou International Expo Center, Hangzhou, China.
          </span>
        </div>
      </div>
    </div>
  </section>
  
  <section id="highlights" style="padding-top: 20px;">
    <div class="container">
      <div class="row">
        <div class="col-md-10 mx-auto">
          <span class="titlesec">Highlights</span>
          <div class="row">
            <div class="col-md-4">
              <img src="./images/highlights/rtx.gif" class="img-fluid fixed-height" alt="Crossformer">
              <p class="text-center">Generalist Manipulation Policy (<a
                  href="https://robotics-transformer-x.github.io/">link</a>)
              </p>
            </div>
            <div class="col-md-4">
              <img src="./images/highlights/crossformer.gif" class="img-fluid fixed-height" alt="Crossformer">
              <p class="text-center">Generalist Navigation Policy (<a
                  href="https://crossformer-model.github.io/">link</a>)
              </p>
            </div>
            <div class="col-md-4">
              <img src="./images/highlights/overcooked.gif" class="img-fluid fixed-height" alt="Human-AI Coordination">
              <p class="text-center">Human-AI Collaboration (<a
                  href="https://sites.google.com/view/e3t-overcooked">link</a>)</p>
            </div>
          </div>
          <div class="row">
            <div class="col-md-4">
              <video autoplay muted loop nocontrols playsinline width="100%">
                <source src="./images/highlights/non_prehensile_tool.mp4" type="video/mp4">
              </video>
              <p class="text-center">Industry Non-Prehensile Tool Use (<a
                  href="https://samanthalhy.github.io/tool_manipulation/">link</a>)</p>
            </div>
            <div class="col-md-4">
              <video autoplay muted loop nocontrols playsinline width="100%">
                <source src="./images/highlights/right_between_v3_x4_hres_caption.mp4" type="video/mp4">
              </video>
              <p class="text-center">Vision-Language-Conditioned Navigation (<a
                  href="https://vlmaps.github.io/">link</a>)</p>
            </div>
            <div class="col-md-4">
              <video autoplay muted loop nocontrols playsinline width="100%">
                <source src="./images/highlights/insert_bit_success_h264.mp4" type="video/mp4">
              </video>
              <p class="text-center">Manipulation for Manufacturing (<a
                  href="https://sichaoliukth.github.io/karm/">link</a>)</p>
            </div>
          </div>
          <div class="row">
            <div class="col-md-4">
              <video autoplay muted loop nocontrols playsinline width="100%">
                <source src="./images/highlights/box_filling_crop.mp4" type="video/mp4">
              </video>
              <p class="text-center">Collaborative Box Filling (<a
                  href="https://www.youtube.com/watch?v=EoG-xTIaw18&ab_channel=HumanRobotInterfacesandInteraction">link</a>)
              </p>
            </div>
            <div class="col-md-4">
              <video autoplay muted loop nocontrols playsinline width="100%">
                <source src="./images/highlights/collaborative_movement.mp4" type="video/mp4">
              </video>
              <p class="text-center">Collaborative Heavy Lifting (<a
                  href="https://www.youtube.com/watch?v=Q3sA6YzTaaE&ab_channel=HumanRobotInterfacesandInteraction">link</a>)
              </p>
            </div>
            <div class="col-md-4">
              <video autoplay muted loop nocontrols playsinline width="100%">
                <source src="./images/highlights/vidbot_teaser.mp4" type="video/mp4">
              </video>
              <p class="text-center">Diffusion-based Mobile Manipulation (<a
                  href="https://hanzhic.github.io/vidbot-project/">link</a>)</p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <hr class="half-rule" />
  <section id="speakers">
    <div class="container">
      <div class="row">
        <div class="col-md-10 mx-auto">
          <span class=titlesec> Invited Speakers</span><br>
          <div class="row">

            <a href="https://www.oiermees.com/" target="_blank">
              <div class="profpic speaker xlarge-1 columns">
                <img src="./images/speakers/oier.jpg" class="figure-img img-fluid ">
                <p class=profname> Oier Mees </p>
                <p class=institution> University of California, Berkeley </p>
              </div>
            </a>

            <a href="https://www.linkedin.com/in/sebasti%C3%A1n-zudaire-48728432a/" target="_blank">
              <div class="profpic speaker xlarge-1 columns">
                <img src="./images/speakers/sebastian.jpg" class="figure-img img-fluid ">
                <p class=profname> Sebastian Zudaire </p>
                <p class=institution> ABB Corporate Research in Sweden </p>
              </div>
            </a>

            <a href="https://yalidu.github.io/" target="_blank">
              <div class="profpic speaker xlarge-1 columns">
                <img src="./images/speakers/yali.jpg" class="figure-img img-fluid ">
                <p class=profname> Yali Du </p>
                <p class=institution> King's College London (KCL) </p>
              </div>
            </a>

            <a href="https://scholar.google.com/citations?user=Y_w632UAAAAJ&hl=en" target="_blank">
              <div class="profpic speaker xlarge-1 columns">
                <img src="./images/speakers/shuai.png" class="figure-img img-fluid ">
                <p class=profname> Shuai Wang </p>
                <p class=institution> Tencent Robotics X</p>
              </div>
            </a>

          </div>
          <div class="row">

            <a href="https://www.polyu.edu.hk/me/people/academic-teaching-staff/david-navarro-alarcon-prof/"
              target="_blank">
              <div class="profpic speaker xlarge-1 columns">
                <img src="./images/speakers/david.jpg" class="figure-img img-fluid ">
                <p class=profname> David Navarro-Alarcon </p>
                <p class=institution> The Hong Kong Polytechnic University (PolyU) </p>
              </div>
            </a>

            <a href="https://mavt.ethz.ch/de/personen/person-detail.MTEyOTE0.TGlzdC8xMzc1LC0xNzA2OTc4MDE3.html" target="_blank">
              <div class="profpic speaker xlarge-1 columns">
                <img src='./images/speakers/stefan.JPG' class="figure-img img-fluid ">
                <p class=profname> Stefan Leutenegger </p>
                <p class=institution> ETH Zurich (ETH) </p>
              </div>
            </a>

            <a href="https://www.iit.it/people-details/-/people/arash-ajoudani" target="_blank">
              <div class="profpic speaker xlarge-1 columns">
                <img src="./images/speakers/arash.jpg" class="figure-img img-fluid ">
                <p class=profname> Arash Ajoudani </p>
                <p class=institution> Italian Institute of Technology (IIT) </p>
              </div>
            </a>

            <a href="https://facultyprofiles.hkust-gz.edu.cn/faculty-personal-page/MA-Shugen/shugenma" target="_blank">
              <div class="profpic speaker xlarge-1 columns">
                <img src="./images/speakers/shugen.jpg" class="figure-img img-fluid ">
                <p class=profname> Shugen Ma </p>
                <p class=institution> Hong Kong University of Science and Technology (Guangzhou) </p>
              </div>
            </a>

            

          </div>
        </div>
      </div>
  </section>



  <hr class="half-rule" />
  <section class="">
    <div class="container" id="schedule">
      <div class="row">
        <div class="col-md-10 mx-auto">
          <span class="titlesec"><span></span>Schedule</span>
          <br>
          <table class="table table-striped">
            <tbody>
              <tr>
                <th style="width: 21%"> Time </th>
              </tr>
              <tr>
                <td style="width: 21%"> 9:00
                  <td />
                <td> Organizers <br> <b> Introductory Remarks </b> </td>
              </tr>
              <tr>
                <td style="width: 21%"> 9:10
                  <td />
                <td> Keynote 1: <a target="_blank" href="https://facultyprofiles.hkust-gz.edu.cn/faculty-personal-page/MA-Shugen/shugenma"> Shugen Ma </a> <br> <b> Bioinspired
                    Intelligent Snake Robots — Embodied Intelligence in
                    a Multi-DOF Robot </b>
                  <br>
                  <a data-toggle="collapse" data-target="#abstract6" class="collapsed abstract" aria-expanded="false">
                    Abstract</a>
                  <div id="abstract6" class="news collapse" style="margin-top: 10px; height: 22px;"
                    aria-expanded="false">
                    Nature systems that have bodies with many degrees of freedom are often considered the ultimate
                    models for machines. To confer the motion performance advantage of animal systems on robotic
                    machines, we conducted in-depth studies on the motion characteristics of biological systems at the
                    biomechanical level. We then used the insights that we obtained to develop intelligent biomimetic
                    robots to achieve "intelligence," "environment adaptation," "flexibility," and "energy-saving." In
                    this talk, I will introduce the bioinspired snake robots we have developed and discuss the evolution
                    of their control methods, from shape-based to neural oscillator-based and then to embodied
                    intelligence, to endow snake robots with motion intelligence.
                  </div>
                </td>
              </tr>
              <tr>
                <td style="width: 21%"> 9:35
                  <td />
                <td> Keynote 2: <a target="_blank" href="https://www.iit.it/people-details/-/people/arash-ajoudani"> Arash Ajoudani </a> <br> <b> Sensor
                    Substitution for Intelligent Manipulation using Machine
                    Learning </b>
                  <br>
                  <a data-toggle="collapse" data-target="#abstract3" class="collapsed abstract" aria-expanded="false">
                    Abstract</a>
                  <div id="abstract3" class="news collapse" style="margin-top: 10px; height: 22px;"
                    aria-expanded="false">
                    Mobile manipulators are increasingly deployed in complex environments, where effective perception
                    and control are crucial for interaction. However, equipping every robot with a full suite of sensors
                    is often impractical due to cost and design constraints. This challenge is particularly evident in
                    non-prehensile manipulation tasks—such as pushing, sliding, or rolling objects—where high-fidelity
                    sensory feedback can significantly impact performance. In this talk, I will discuss how AI-driven
                    approaches can enable robots to adapt to varying sensor configurations and improve non-prehensile
                    manipulation. A key challenge arises when a robot trained with rich sensory inputs, such as tactile
                    skin, needs to be replaced or augmented by a system with a more limited sensor set, like LiDAR or
                    RGB-D cameras. To address this, we propose a machine learning framework that allows robots to
                    substitute missing sensory inputs by learning a mapping between available perception data and the
                    information provided by absent sensors. Beyond sensor substitution, AI models can enhance
                    non-prehensile manipulation by learning robust policies that generalize across different sensing
                    modalities and task variations. I will present experimental results demonstrating how mobile
                    manipulators can leverage AI to perform complex pushing tasks with limited sensing, achieving
                    performance comparable to or even exceeding that of robots using direct tactile feedback. This
                    approach paves the way for more adaptable and cost-effective robotic systems capable of learning and
                    optimizing their interactions in diverse environments.
                  </div>
                </td>
              </tr>

              <tr>
                <td style="width: 21%"> 10:00
                  <td />
                <td> Spotlight Talk I</td>
              </tr>
              <tr>
                <td style="width: 21%"> 10:10
                  <td />
                <td> Coffee Break, Socializing, Posters </td>
              </tr>
              <tr>
                <td style="width: 21%"> 10:40
                  <td />
                <td> Keynote 3: <a target="_blank" href="https://mavt.ethz.ch/de/personen/person-detail.MTEyOTE0.TGlzdC8xMzc1LC0xNzA2OTc4MDE3.html"> Stefan Leutenegger </a> <br> <b>
                    Real-World Mobile Robotics: from Perception to Navigation and
                    Control in the Age of AI </b>
                  <br>
                  <a data-toggle="collapse" data-target="#abstract5" class="collapsed abstract" aria-expanded="false">
                    Abstract</a>
                  <div id="abstract5" class="news collapse" style="margin-top: 10px; height: 22px;"
                    aria-expanded="false">
                    To power the next generation mobile robots and drones, the field of spatial perception has made much
                    progress from robust multi-sensor SLAM to dense, semantic, and object-level maps, with the aim of
                    understanding open-ended environments as a basis for mobile robot navigation and environment
                    interaction. I will show recent progress in reliable and real-time state estimation and 3D scene
                    understanding using vision, LiDAR, IMUs, and more. Scenes to be reconstructed may contain dynamic
                    objects and even people, whose poses, postures, and motions we can estimate in a tightly-coupled
                    manner. In our works, we fully embrace the power of machine learning-based approaches, but typically
                    integrated in modular, complex robotic systems that may include model-based methods as well. Our
                    approaches are demonstrated as crucial enablers of a range of robot applications, from mobile
                    manipulation on construction sites to dronesexploring obstructed indoor spaces or flying through the
                    forest.
                  </div>
                </td>
              </tr>
              <tr>
                <td style="width: 21%"> 11:05
                  <td />
                <td> Keynote 4: <a target="_blank" href="https://yalidu.github.io/"> Yali Du </a> <br> <b> RL/LLM for
                    Multi-Agent Decision-Making and Robotics </b>
                  <br>
                  <a data-toggle="collapse" data-target="#abstract7" class="collapsed abstract" aria-expanded="false">
                    Abstract</a>
                  <div id="abstract5" class="news collapse" style="margin-top: 10px; height: 22px;"
                    aria-expanded="false">
                    From collaborative industrial robots to personal AI assistants, the integration of AI into our daily
                    lives highlights the critical need for effective and reliable coordination among agents, as well as
                    between agents and humans. This challenge centers on creating agents that not only align with user
                    intentions but also possess the flexibility to adapt to evolving circumstances, such as the
                    introduction of novel agents. The pursuit of multi-agent cooperation extends beyond individual
                    interactions to encompass broader societal considerations. In this talk, I will discuss the
                    challenges of cooperative AI, and our contributions on multi-agent cooperation, human-ai
                    coordination and cooperative alignments.
                  </div>
                </td>
              </tr>
              <tr>
                <td style="width: 21%"> 11:30
                  <td />
                <td> Panel Discussion <br> <b>Panelists:</b> Shugen Ma, Arash Ajoudani, Stefan Leutenegger, Yali Du
                   </td>
              </tr>
              <tr>
                <td style="width: 21%"> 12:00
                  <td />
                <td> Lunch </td>
              </tr>

              <tr>
                <td style="width: 21%"> 13:00
                  <td />
                <td> Keynote 5: <a target="_blank" href="https://scholar.google.com/citations?user=Y_w632UAAAAJ&hl=en"> Shuai Wang </a> <br> <b> Eight-year’s Journey of Mobile Robots’ Design and Control: from Model-based Methods to The Reinforcement Learning Approach </b>
                  <br>
                  <a data-toggle="collapse" data-target="#abstract1" class="collapsed abstract" aria-expanded="false">
                    Abstract</a>
                  <div id="abstract1" class="news collapse" style="margin-top: 10px; height: 22px;"
                    aria-expanded="false">
                    Researchers have been working on the design of mobile robot body configurations and control algorithms for
                    decades. This report describes Tencent Robotics X's research journey on robot body design, control
                    algorithms, application background, and ecosystem construction over the past eight years. The content includes
                     the wheeled balancing robot Robicycle, the quadruped robot Max, the wheel-legged robot Ollie, the industrial
                      intelligent inspection robot, and the elderly care robot The Five. Recent works also include the 
                      data-driven approaches and applications based on the multi-robot platforms.
                  </div>
                </td>
              </tr>
              <tr>
                <td style="width: 21%"> 13:25
                  <td />
                <td> Keynote 6: <a target="_blank" href="https://www.polyu.edu.hk/me/people/academic-teaching-staff/david-navarro-alarcon-prof/"> David Navarro-Alarcon (Hoi-Yin Lee) </a><br> <b>
                    Non-Prehensile Tool-Object Manipulation by Integrating LLM-
                    Based Planning and Manoeuvrability-Driven Controls</b>
                  <br>
                  <a data-toggle="collapse" data-target="#abstract3" class="collapsed abstract" aria-expanded="false">
                    Abstract</a>
                  <div id="abstract3" class="news collapse" style="margin-top: 10px; height: 22px;"
                    aria-expanded="false">
                    Tool use isn't just for humans anymore — we've long known that animals like crows can manipulate
                    objects with remarkable skill. Yet getting robots to handle tools with similar dexterity remains a
                    major challenge. In this talk, I'll present our current efforts in combining Large Language Models
                    with visual feedback to enable robots to understand and execute tool-based tasks. Our method
                    translates natural language instructions into concrete motion sequences, guided by a new tool
                    affordance model that helps the robot navigate even tight spaces. I'll demonstrate how this hybrid
                    approach bridges the gap between human commands and robotic actions, bringing us closer to more
                    adaptable and capable robotic systems. Through experimental results, I'll show how our methodology
                    performs across different manipulation scenarios, highlighting both its current capabilities and
                    future potential.
                  </div>
                </td>
              </tr>

              <tr>
                <td style="width: 21%"> 13:50
                  <td />
                <td> Spotlight Talk II</td>
              </tr>
              <tr>
                <td style="width: 21%"> 14:00
                  <td />
                <td> Coffee Break, Socializing, Posters </td>
              </tr>
              <tr>
                <td style="width: 21%"> 14:30
                  <td />
                <td> Keynote 7: <a target="_blank" href="https://www.oiermees.com/"> Oier Mees </a> <br> <b> Generalist
                    Robots in the Era of AI </b>
                  <br>
                  <a data-toggle="collapse" data-target="#abstract1" class="collapsed abstract" aria-expanded="false">
                    Abstract</a>
                  <div id="abstract1" class="news collapse" style="margin-top: 10px; height: 22px;"
                    aria-expanded="false">
                    Enabling robots to evolve from specialized, task-specific systems to versatile, adaptive generalist
                    agents is an open and challenging problem. The rapid advancements in generative models, such as
                    diffusion models and multimodal foundation models, have shown great potential in the development of
                    generalist robots capable of performing a wide range of tasks across diverse environments. One key
                    aspect of a generalist robot is the embodied multimodal intelligence, which emphasizes the
                    comprehension of language and multisensory inputs, the grounding across different modalities, and
                    the generation of action in environments based on these inputs. Frontier techniques in pre-training,
                    post-training, reinforcement learning (RL), chain-of-thought reasoning, and simulation for
                    multimodal robotic foundation models will be covered in this workshop, providing more insights on
                    training generalizable and adaptive generative policies to the community.
                  </div>
                </td>
              </tr>
              <tr>
                <td style="width: 21%"> 14:55
                  <td />
                <td> Keynote 8: <a target="_blank" href="https://www.linkedin.com/in/sebasti%C3%A1n-zudaire-48728432a/"> Sebastian Zudaire </a> <br> <b> ABB
                    Research Accelerates AI-enabled Robotic Applications and
                    Industrial Automation </b>
                  <br>
                  <a data-toggle="collapse" data-target="#abstract8" class="collapsed abstract" aria-expanded="false">
                    Abstract</a>
                  <div id="abstract6" class="news collapse" style="margin-top: 10px; height: 22px;"
                    aria-expanded="false">
                    Recent developments in the fields of AI and Generative AI have enabled a new level of interaction
                    between user and robot systems. For the first time the users can explain the task for the robot in
                    fully unstructured natural language and robot motion can be generated accordingly. In this talk I
                    will present activities conducted in ABB Corporate Research Center in Sweden that highlight
                    different mechanisms in which AI and Generative AI can be introduced into robotic applications and
                    industrial automation.</div>
                </td>
              </tr>
              <tr>
                <td style="width: 21%"> 15:20
                  <td />
                <td> Panel Discussion <br> <b>Panelists:</b> Shuai Wang, David Navarro-Alarcon (Hoi-Yin Lee), Oier Mees, Sebastian Zudaire
                </td>
              </tr>

              <tr>
                <td style="width: 21%"> 16:00
                  <td />
                <td> Summary and interactive discussions <br>
              </tr>
              <tr>
                <td style="width: 21%"> 16:30
                  <td />
                <td> Organizers <br> <b> Closing Remarks </b>
                </td>
              </tr>
            </tbody>
          </table>
        </div>
      </div>
    </div>
  </section>

  <hr class="half-rule"/>
    <section id="papers">
      <div class="container">
        <div class="row">
          <div class="col-md-10 mx-auto">
            <span class=titlesec>Accepted Papers</span><br>
            The following papers have been accepted for poster presentation and a spotlight talk at the workshop.

            Authors should print and bring their own posters and at least one author must be present during the Poster Session. Posters discussions can be continued during the coffee break and the lunch. Posters should adhere to the IROS poster guidelines (no larger than 36 inches wide and 48 inches in height, <a target="_blank"
            href="https://www.iros25.org/InstructionsOnAuthorsPresentationMaterials">link</a>). The author of the accepted paper can promote their work in a 2-minute spotlight talk. The authors are requested to submit a 1-slide presentation (pdf or ppt) and 1-minute video for their spotlight talk before the 20th of October (23:59) by sending an email to <a href="mailto:genai4robotics@gmail.com">genai4robotics@gmail.com</a> or <a href="mailto:sichao.liu@epfl.ch.com">sichao.liu@epfl.ch</a>. The organizers will display the slide during the spotlight talk.

            <ul class="listpapers">
              <li>
                (Paper ID #1) ROBOVERSE: Towards a Unified Platform, Dataset and Benchmark for Scalable and Generalizable Robot Learning 
                <span style="color: #ff8c00">(spotlight)</span>
                <br>
                <span class="authorname">
                Haoran Geng, Feishi Wang, Songlin Wei, Yuyang Li, Bangjun Wang, Boshi An,
                Charlie Tianyue Cheng, Haozhe Lou, Peihao Li, Yen-Jen Wang, Yutong Liang, Dylan Goetting,
                Chaoyi Xu, Haozhe Chen, Yuxi Qian, Yiran Geng, Jiageng Mao, Weikang Wan, Mingtong Zhang,
                Jiangran Lyu, Siheng Zhao, Jiazhao Zhang, Jialiang Zhang, Chengyang Zhao, Haoran Lu,
                Yufei Ding, Ran Gong, Yuran Wang, Yuxuan Kuang, Ruihai Wu, Baoxiong Jia, Carlo Sferrazza,
                Hao Dong, Siyuan Huang, Yue Wang, Jitendra Malik, Pieter Abbeel
                </span>
               <br>
                <a data-toggle="collapse" data-target="#abstract1" class="collapsed abstract" aria-expanded="false"> Abstract</a>
                  <div id="abstract1" class="news collapse" style="margin-top: 10px; height: 22px; font-weight: normal;" aria-expanded="false">
                    Data scaling and standardized evaluation benchmarks have driven significant advances in natural language processing and computer vision. However, robotics faces unique
                    challenges in scaling data and establishing reliable evaluation
                    protocols. Collecting real-world robotic data is resource-intensive
                    and inefficient, while benchmarking in real-world scenarios
                    remains highly complex. Synthetic data and simulation offer
                    promising alternatives, yet existing efforts often fall short in data
                    quality, diversity, and benchmark standardization. To address
                    these challenges, we introduce ROBOVERSE , a comprehensive
                    framework comprising a simulation platform, a synthetic dataset,
                    and unified benchmarks. Our simulation platform supports
                    multiple simulators and robotic embodiments, enabling seamless
                    transitions between different environments. The synthetic dataset,
                    featuring high-fidelity physics and photorealistic rendering, is
                    constructed through multiple approaches including migration
                    from public datasets, policy rollout, and motion planning,
                    etc. enhanced by data augmentation. Additionally, we propose
                    unified benchmarks for imitation learning and reinforcement
                    learning, enabling consistent evaluation across different levels of
                    generalization. At the core of the simulation platform is M ETA S IM ,
                    an infrastructure that abstracts diverse simulation environments
                    into a universal interface. It restructures existing simulation
                    environments into a simulator-agnostic configuration system, as
                    well as an API aligning different simulator functionalities, such
                    as launching simulation environments, loading assets with initial
                    states, stepping the physics engine, etc. This abstraction ensures
                    interoperability and extensibility. Comprehensive experiments
                    demonstrate that ROBOVERSE enhances the performance of imi-
                    tation learning, reinforcement learning, and world model learning,
                    improving sim-to-real transfer. These results validate the reliability
                    of our dataset and benchmarks, establishing RoboVerse as a robust
                    solution for advancing simulation-assisted robot learning. Code
                    and dataset can be found at: https://roboverseorg.github.io/.
                  </div>
                  <br>
              </li>
              <li>
                (Paper ID #2) RobotGPT-Weld: A Foundation-Model-Driven Pipeline from CAD
                Weld Seam Extraction to Robotic Program Generation in Shipyards
                <!-- <a class="linkpaper" href="./docs/corl_llhomerobots_2024-cr_paper_1.pdf" target="_blank"> [paper]</a> -->
                <!-- <span style="color: #ff8c00">(spotlight)</span> -->
                <br>
                <span class="authorname">
                Xiwei Wu, Wei Wu, Tian Li, Changjin Yan, Yuda Cao, George Q. Huang
                </span>
               <br>
                <a data-toggle="collapse" data-target="#abstract2" class="collapsed abstract" aria-expanded="false"> Abstract</a>
                  <div id="abstract2" class="news collapse" style="margin-top: 10px; height: 22px; font-weight: normal;" aria-expanded="false">
                    Small and mid-sized shipyards still rely on manual
                    robot programming for welding, which increases labor and training costs, and leads to inconsistent quality. To achieve welding
                    automation and close the gap from seam extraction to executable
                    code with minimal manual work, a large-language-model (LLM)-
                    based CAD-to-robot workflow is introduced with two coordinated
                    pipelines, TrajectoryGPT and RobotGPT. TrajectoryGPT parses
                    CAD models, detects weld seams, infers their geometry, and plans
                    the welding sequence using shipbuilding knowledge, then forms
                    weld paths. RobotGPT takes these paths and generates vendor-specific robot programs and welding settings, performs motion
                    and safety checks, applies postprocessing and static analysis.
                    Case studies show accurate seam extraction, large reductions
                    in programming time, and reliable execution on a UR5e robot.
                    The proposed approach offers a practical route to low-cost, fast-
                    deployable welding automation in small and mid-sized shipyards,
                    and builds a foundation for scaling to more ship types and
                    processes.
                  </div>
                  <br>
              </li>
              <li>
                (Paper ID #3) An AR-Guided Framework for Low-Code Robotic
                Drilling Using Large Language Models
                <!-- <a class="linkpaper" href="./docs/corl_llhomerobots_2024-cr_paper_1.pdf" target="_blank"> [paper]</a>
                <span style="color: #ff8c00">(spotlight)</span> -->
                <br>
                <span class="authorname">
                Wenhang Dong, Pai Zheng
                </span>
               <br>
                <a data-toggle="collapse" data-target="#abstract3" class="collapsed abstract" aria-expanded="false"> Abstract</a>
                  <div id="abstract3" class="news collapse" style="margin-top: 10px; height: 22px; font-weight: normal;" aria-expanded="false">
                    This paper introduces an intelligent robotic drilling
                    system driven by Augmented Reality (AR) and natural language,
                    designed to address the challenges of conventional robotic drilling
                    solutions in large-scale aircraft manufacturing. The system utilizes AR technology for intuitive point calibration and fine-
                    tuning, and leverages a Large Language Model (LLM) to convert
                    natural language commands into machine-executable process
                    parameters in real-time. This ultimately establishes a low-code,
                    highly flexible, and high-precision automated drilling framework.
                    The approach not only significantly reduces the complexity
                    and deployment costs of human-robot collaboration but also
                    advances the deep application of virtual-real fusion technology
                    in manufacturing field.
                  </div>
                  <br>
              </li>
              <li>
                (Paper ID #4) Dual robots collaborative knotting of metal wire
                based  on multi-modal data and metal rebound model
                <!-- <a class="linkpaper" href="./docs/corl_llhomerobots_2024-cr_paper_1.pdf" target="_blank"> [paper]</a> -->
                <!-- <span style="color: #ff8c00">(spotlight)</span> -->
                <br>
                <span class="authorname">
                Yiyang Hu, Bitao Yao, Wenjun Xu
                </span>
               <br>
                <a data-toggle="collapse" data-target="#abstract4" class="collapsed abstract" aria-expanded="false"> Abstract</a>
                  <div id="abstract4" class="news collapse" style="margin-top: 10px; height: 22px; font-weight: normal;" aria-expanded="false">
                    The tasks of operating deformable linear objects
                    (DLOs) are mostly completed manually. This article deals with
                    operations of DLOs, such as knotting of metal wire, adopting
                    two collaborative robots to accomplish the knotting processes of
                    flexible wires. Binocular vision is applied to extract wire
                    features, with 3D reconstruction achieved through particle
                    swarm optimization algorithm. For trajectory planning, a
                    rebound prediction model is constructed based on the material
                    properties of the wire, integrated with acquired visual
                    information to realize trajectory planning with obstacle
                    avoidance for dual robots. To enhance knotting quality, hybrid
                    force-position control is implemented for the robotic control
                    incorporating fuzzy logic. Experimental results show the
                    effectiveness of the knotting method.
                  </div>
                  <br>
              </li>
              <li>
                (Paper ID #5) LEMMo-Plan: LLM-Enhanced Learning from Multi-Modal Demonstration
                for Planning Sequential Contact-Rich Manipulation Tasks
                <!-- <a class="linkpaper" href="./docs/corl_llhomerobots_2024-cr_paper_1.pdf" target="_blank"> [paper]</a> -->
                <span style="color: #ff8c00">(spotlight)</span>
                <br>
                <span class="authorname">
                Kejia Chen, Zheng Shen, Yue Zhang, Lingyun Chen,
                Fan Wu, Zhenshan Bing, Sami Haddadin, Alois Knoll
                </span>
               <br>
                <a data-toggle="collapse" data-target="#abstract5" class="collapsed abstract" aria-expanded="false"> Abstract</a>
                  <div id="abstract5" class="news collapse" style="margin-top: 10px; height: 22px; font-weight: normal;" aria-expanded="false">
                    Large Language Models (LLMs) have gained
                    popularity in task planning for long-horizon manipulation
                    tasks. To enhance the validity of LLM-generated plans, visual
                    demonstrations and online videos have been widely employed
                    to guide the planning process. However, for manipulation tasks
                    involving subtle movements but rich contact interactions, visual
                    perception alone may be insufficient for the LLM to fully
                    interpret the demonstration. Additionally, visual data provides
                    limited information on force-related parameters and conditions,
                    which are crucial for effective execution on real robots.
                    In this paper, we introduce LEMMo-Plan, an in-context
                    learning framework that incorporates tactile and force-torque
                    information from human demonstrations to enhance LLMs’
                    ability to generate plans for new task scenarios. We propose
                    a bootstrapped reasoning pipeline that sequentially integrates
                    each modality into a comprehensive task plan. This task
                    plan is then used as a reference for planning in new task
                    configurations. Real-world experiments on two different sequential manipulation tasks demonstrate the effectiveness of
                    our framework in improving LLMs’ understanding of multi-
                    modal demonstrations and enhancing the overall planning
                    performance. More materials are available on our project
                    website: lemmo-plan.github.io/LEMMo-Plan/.
                  </div>
                  <br>
              </li>
              <li>
                (Paper ID #6) LOOP: Language Oriented Object Packing with
                Diffusion Models
                <!-- <a class="linkpaper" href="./docs/corl_llhomerobots_2024-cr_paper_1.pdf" target="_blank"> [paper]</a>
                <span style="color: #ff8c00">(spotlight)</span> -->
                <br>
                <span class="authorname">
                Anurag Maurya, Shashwat Gupta, Sandip Das, Shivam Vats, Ravi Prakash
                </span>
               <br>
                <a data-toggle="collapse" data-target="#abstract6" class="collapsed abstract" aria-expanded="false"> Abstract</a>
                  <div id="abstract6" class="news collapse" style="margin-top: 10px; height: 22px; font-weight: normal;" aria-expanded="false">
                    The irregular bin packing problem is a well-known
                    NP-hard challenge with significant applications in logistics and
                    manufacturing. Traditional methods often rely on hand-crafted
                    heuristics, which can be inflexible in accommodating complex
                    or customizable preferences. Conversely, purely learning-based
                    approaches often require task-specific training data. We propose
                    LOOP: a physics-aware bin packing framework that combines
                    diffusion sampling with simulator-based physics integration. We
                    further extend the framework by leveraging pretrained large
                    language models (LLMs) as interpreters of natural language
                    preferences. We use a barrier function formulation to encode
                    object preferences. The LLM defines preferred placements and
                    constrained regions for each object. LOOP allows for zero-
                    shot adaptation to user preferences while maintaining physically
                    plausible packing solutions.
                  </div>
                  <br>
              </li>
              <li>
                (Paper ID #7) Fast ECoT: Efficient Embodied Chain-of-Thought via Thoughts Reuse
                <!-- <a class="linkpaper" href="./docs/corl_llhomerobots_2024-cr_paper_1.pdf" target="_blank"> [paper]</a> -->
                <span style="color: #ff8c00">(spotlight)</span>
                <br>
                <span class="authorname">
                Zhekai Duan, Yuan Zhang, Shikai Geng, Gaowen Liu, Joschka Boedecker, Chris Xiaoxuan Lu
                </span>
               <br>
                <a data-toggle="collapse" data-target="#abstract7" class="collapsed abstract" aria-expanded="false"> Abstract</a>
                  <div id="abstract7" class="news collapse" style="margin-top: 10px; height: 22px; font-weight: normal;" aria-expanded="false">
                    Embodied Chain-of-Thought (ECoT) reasoning
                    enhances vision-language-action (VLA) models by improving
                    performance and interpretability through intermediate reasoning steps. However, its sequential autoregressive token generation introduces significant inference latency, limiting real-time deployment. We propose Fast ECoT, an inference-time
                    acceleration method that exploits the structured and repetitive
                    nature of ECoT to (1) cache and reuse high-level reasoning
                    across timesteps and (2) parallelise the generation of modular
                    reasoning steps. Additionally, we introduce an asynchronous
                    scheduler that decouples reasoning from action decoding, further boosting responsiveness. Fast ECoT requires no model
                    changes or additional training and easily integrates into existing
                    VLA pipelines. Experiments in both simulation (LIBERO) and
                    real-world robot tasks show up to a 7.5× reduction in latency
                    with comparable or improved task success rate and reasoning
                    faithfulness, bringing ECoT policies closer to practical real-time
                    deployment. The code will be released upon acceptance.
                  </div>
                  <br>
              </li>
              <li>
                (Paper ID #8) Collision-Aware Motion Planning with Time-Varying SDF for
                Robot-Assisted Multi-Axis Additive Manufacturing
                <!-- <a class="linkpaper" href="./docs/corl_llhomerobots_2024-cr_paper_1.pdf" target="_blank"> [paper]</a> -->
                <span style="color: #ff8c00">(spotlight)</span>
                <br>
                <span class="authorname">
                Jiasheng Qu, Zhikai Shen, Guoxin Fang
                </span>
               <br>
                <a data-toggle="collapse" data-target="#abstract8" class="collapsed abstract" aria-expanded="false"> Abstract</a>
                  <div id="abstract8" class="news collapse" style="margin-top: 10px; height: 22px; font-weight: normal;" aria-expanded="false">
                    Collision checking and avoidance are critical challenges in robotic motion planning for multi-axis additive man-
                    ufacturing (MAAM) due to the dynamically changing printing
                    object. To address this, we propose the Time-Varying Signed
                    Distance Field (TV-SDF), a training-free method for dynamic
                    object representation via neural field interpolation. The TV-
                    SDF enables efficient modeling and differentiable motion planning for collision avoidance, ensuring successful collision-free
                    fabrication. Additionally, by training a neural quaternion field
                    that integrates motion and fabrication constraints, the motion is
                    optimized for smooth, collision-free planning while maintaining
                    the support-free condition. Both computational and physical
                    experiments demonstrate the effectiveness of the proposed
                    method. Project page: https://qjiasheng.github.io/
                    crml/inf3dp/.
                  </div>
                  <br>
              </li>
              <li>
                (Paper ID #9) Generative AI-Driven Robot Skill Learning and
                Human-Guided Transfer for Smart Assembly
                <!-- <a class="linkpaper" href="./docs/corl_llhomerobots_2024-cr_paper_1.pdf" target="_blank"> [paper]</a>
                <span style="color: #ff8c00">(spotlight)</span> -->
                <br>
                <span class="authorname">
                Duidi Wu, Qianyou Zhao, Pai Zheng, Jin Qi, Jie Hu
                </span>
               <br>
                <a data-toggle="collapse" data-target="#abstract9" class="collapsed abstract" aria-expanded="false"> Abstract</a>
                  <div id="abstract9" class="news collapse" style="margin-top: 10px; height: 22px; font-weight: normal;" aria-expanded="false">
                   The emergence of Industry 5.0 emphasizes human-
                    centric smart manufacturing, yet achieving natural and adaptive
                    human-robot interaction remains challenging. Current advances
                    in generative AI (GenAI) such as large language models provide
                    promising ways for language-guided planning and decision-
                    making, but their application in contact-rich, safety-critical
                    industrial tasks is still limited. This paper explores how GenAI
                    combined with embodied agents can overcome these limitations.
                    A set of paradigms are proposed: vision–language–action model
                    for integrated perception and execution, LLM-driven reward
                    generation for assembly skill learning, and VR-assisted generative
                    imitation for human–robot skill transfer. The effectiveness is
                    demonstrated in simulation and real-world settings, highlighting
                    their practical applicability.
                  </div>
                  <br>
              </li>
              <li>
                (Paper ID #10) LLM-assisted cross-modal deep learning for spatial
                disassembly constraint modelling in
                remanufacturing bolster springs
                <!-- <a class="linkpaper" href="./docs/corl_llhomerobots_2024-cr_paper_1.pdf" target="_blank"> [paper]</a>
                <span style="color: #ff8c00">(spotlight)</span> -->
                <br>
                <span class="authorname">
                Wupeng Deng, Daode Zhang, Kaiwen Jiang, Yongjing Wang, Duc Truong Pham
                </span>
               <br>
                <a data-toggle="collapse" data-target="#abstract10" class="collapsed abstract" aria-expanded="false"> Abstract</a>
                  <div id="abstract10" class="news collapse" style="margin-top: 10px; height: 22px; font-weight: normal;" aria-expanded="false">
                    Autonomous robotic disassembly requires
                    accurate modelling of spatial disassembly constraints. This
                    paper proposes an LLM-assisted cross-modal deep learning
                    method for remanufacturing, in which the CAD models and
                    captured images are linked by leveraging the semantic
                    modelling capability of LLMs. We first develop a two-stream
                    deep learning module to extract labels and positions of
                    components based on the captured images for products and
                    section views for CAD models. The image-based detected results
                    and CAD-based detected results are used to construct the
                    semantic graph in the same comparable space using LLMs.
                    Finally, the CAD-based semantic graph that is most similar to
                    the image-based semantic graph is employed to retrieve the
                    spatial disassembly constraints from the knowledge base. The
                    proposed method is implemented on recycling used bolster
                    springs, demonstrating the strong capability in constructing and
                    comparing semantic graphs in the cross-modal space. With the
                    LLM techniques, the disassembly constraints can be
                    automatically constructed to enable robotic disassembly.
                  </div>
                  <br>
              </li>
              <li>
                (Paper ID #11) InstructTODG: A Multimodal LLMs-driven
                Approach for Task-oriented Dexterous Grasping in
                Unstructured Human-Robot Collaborative
                Manufacturing
                <!-- <a class="linkpaper" href="./docs/corl_llhomerobots_2024-cr_paper_1.pdf" target="_blank"> [paper]</a> -->
                <span style="color: #ff8c00">(spotlight)</span>
                <br>
                <span class="authorname">
                Benhua Gao, Tian Wang, Benhua Gao, Tian Wang, Zeyuan Ren, Pai Zheng
                </span>
               <br>
                <a data-toggle="collapse" data-target="#abstract11" class="collapsed abstract" aria-expanded="false"> Abstract</a>
                  <div id="abstract11" class="news collapse" style="margin-top: 10px; height: 22px; font-weight: normal;" aria-expanded="false">
                    Human-robot collaborative manufacturing (HRCM)
                    requires robots not only to perform stable and precise grasps,
                    but also to align with task-specific human instructions. However, most existing dexterous grasping approaches emphasize
                    geometric stability while neglecting instruction alignment and
                    affordance awareness. To address this challenge, we propose
                    InstructTODG, a multimodal large language model (MLLM)-
                    driven framework for task-oriented dexterous grasping in unstructured HRC environments. InstructTODG first employs an
                    MLLM-based reasoning module enhanced with visual prompts
                    to interpret human instructions and identify target objects
                    with associated affordances. A zero-shot coarse-to-fine 6D object
                    pose estimation network is then introduced to recover object
                    geometry and spatial pose. Finally, a language-guided affordance
                    grounding module segments task-relevant regions, which are used
                    as conditions for generative models to synthesize coordinated
                    wrist poses and finger joint configurations. Experimental results
                    in both simulation and real-world scenarios demonstrate that
                    InstructTODG enables instruction-aligned, functional, and stable
                    dexterous grasps, significantly enhancing robot manipulation
                    capabilities and advancing human-robot collaboration in complex
                    manufacturing tasks.
                  </div>
                  <br>
              </li>
              <li>
                (Paper ID #12) MetaFold: Language-Guided Multi-Category Garment Folding
                Framework via Trajectory Generation and Foundation Model
                <!-- <a class="linkpaper" href="./docs/corl_llhomerobots_2024-cr_paper_1.pdf" target="_blank"> [paper]</a> -->
                <span style="color: #ff8c00">(spotlight)</span>
                <br>
                <span class="authorname">
                Haonan Chen, Junxiao Li, Ruihai Wu, Yiwei Liu, Yiwen Hou, Zhixuan Xu,
                Jingxiang Guo, Chongkai Gao, Zhenyu Wei, Shensi Xu, Jiaqi Huang, Lin Shao
                </span>
               <br>
                <a data-toggle="collapse" data-target="#abstract12" class="collapsed abstract" aria-expanded="false"> Abstract</a>
                  <div id="abstract12" class="news collapse" style="margin-top: 10px; height: 22px; font-weight: normal;" aria-expanded="false">
                  Garment folding is a common yet challenging task
                  in robotic manipulation. The deformability of garments leads
                  to a vast state space and complex dynamics, which complicates precise and fine-grained manipulation. In this paper, we
                  present MetaFold, a unified framework that disentangles task
                  planning from action prediction and learns each independently
                  to enhance model generalization. It employs language-guided
                  point cloud trajectory generation for task planning and a low-
                  level foundation model for action prediction. This structure
                  facilitates multi-category learning, enabling the model to adapt
                  flexibly to various user instructions and folding tasks. We also
                  construct a large-scale MetaFold dataset comprising folding
                  point cloud trajectories for a total of 1210 garments across
                  multiple categories, each paired with corresponding language
                  annotations. Extensive experiments demonstrate the superiority
                  of our proposed framework. Supplementary materials are
                  available on our website: https://meta-fold.github.io/.
                  </div>
                  <br>
              </li>


              <li>
                (Paper ID #13) MetricNet: Recovering Metric Scale in Generative Navigation Policies
                <!-- <a class="linkpaper" href="./docs/corl_llhomerobots_2024-cr_paper_1.pdf" target="_blank"> [paper]</a> -->
                <span style="color: #ff8c00">(spotlight)</span>
                <br>
                <span class="authorname">
                Abhijeet Nayak, Débora N.P. Oliveira, Samiran Gode, Cordelia Schmid, Wolfram Burgard
                </span>
               <br>
                <a data-toggle="collapse" data-target="#abstract13" class="collapsed abstract" aria-expanded="false"> Abstract</a>
                  <div id="abstract13" class="news collapse" style="margin-top: 10px; height: 22px; font-weight: normal;" aria-expanded="false">
                  Generative navigation policies have made rapid
                  progress in improving end-to-end learned navigation. Despite
                  their promising results, this paradigm has two structural
                  problems. First, the sampled trajectories exist in an abstract,
                  unscaled space without metric grounding. Second, the control
                  strategy discards the full path, instead moving directly towards
                  a single waypoint. This leads to short-sighted and unsafe
                  actions, moving the robot towards obstacles that a complete
                  and correctly scaled path would circumvent. To address these
                  issues, we propose MetricNet, an effective add-on for generative
                  navigation that predicts the metric distance between waypoints,
                  grounding policy outputs in real-world coordinates. We evaluate
                  our method in simulation with a new benchmarking framework
                  and show that executing MetricNet-scaled waypoints significantly improves both navigation and exploration performance.
                  Beyond simulation, we further validate our approach in real-world experiments. Finally, we propose MetricNav, which
                  integrates MetricNet into a navigation policy to guide the robot
                  away from obstacles while still moving towards the goal.
                  </div>
                  <br>
              </li>
              <li>
                (Paper ID #14) A Modular Vision-Language-Action Framework for
                Robotic Task Automation in Indoor Environments
                <!-- <a class="linkpaper" href="./docs/corl_llhomerobots_2024-cr_paper_1.pdf" target="_blank"> [paper]</a>
                <span style="color: #ff8c00">(spotlight)</span> -->
                <br>
                <span class="authorname">
                Anindya Jana, Snehasis Banerjee, Arup Sadhu, Ranjan Dasgupta
                </span>
               <br>
                <a data-toggle="collapse" data-target="#abstract14" class="collapsed abstract" aria-expanded="false"> Abstract</a>
                  <div id="abstract14" class="news collapse" style="margin-top: 10px; height: 22px; font-weight: normal;" aria-expanded="false">
                  This paper presents an integrated system for Vision-
                  Language-Action (VLA) tasks, designed to enable an autonomous
                  mobile robot to perform complex operations in structured in-door environments based on natural language instructions. Our
                  framework employs a modular architecture that orchestrates
                  environment mapping, language processing, and navigation. The
                  system operates in two parallel streams: a perception pipeline
                  that constructs a semantic voxel map from real-time camera
                  feeds using OwlViT embeddings, and a language pipeline that
                  classifies user commands with a Vision-Language Model (VLM).
                  The mapping phase is time-constrained to ensure responsiveness,
                  proceeding with a partial map if a predefined exploration limit is
                  reached. The classified query is then grounded in the geometric
                  and semantic context of the map to generate a detailed prompt
                  for the VLM. This yields an actionable output, demonstrating a
                  capable solution for flexible robotic automation.
                  </div>
                  <br>
              </li>
              <li>
                (Paper ID #15) Towards Logic-Aware Manipulation: Knowledge
                  Primitive for VLMs in Smart Manufacturing
                <!-- <a class="linkpaper" href="./docs/corl_llhomerobots_2024-cr_paper_1.pdf" target="_blank"> [paper]</a> -->
                <span style="color: #ff8c00">(spotlight)</span>
                <br>
                <span class="authorname">
                Suchang Chen, Daqiang Guo
                </span>
               <br>
                <a data-toggle="collapse" data-target="#abstract15" class="collapsed abstract" aria-expanded="false"> Abstract</a>
                  <div id="abstract15" class="news collapse" style="margin-top: 10px; height: 22px; font-weight: normal;" aria-expanded="false">
                    Current pipelines for vision-language models
                    (VLMs) in robotic manipulation unlocked broad semantic generalization with appearance cues and generic instructions, while
                    omitting the process parameters that make contact-rich manipulation succeed in manufacturing, including interface mecha-
                    nism, contact modality, trajectory shaping, precision bands, and
                    force/impedance. We present an object-centric manipulation logic
                    schema, serialize it as an 8-field tuple τ and define it as a first-class knowledge signal. The schema enables two concrete uses:
                    at training time, taxonomy-tagged augmentation that teaches
                    models how to operate device interfaces; at test time, logic-aware prompting with retrieval from a compact knowledge
                    base to inject instance-specific constraints. This position paper
                    specifies the schema, sketches a minimal pipeline, and outlines
                    a compact evaluation protocol targeting first-try success, fewer
                    force-limit violations, and clearer failure attribution on novel
                    devices. The schema covers both contact-rich and precision-sensitive tasks and is designed for practical deployment in
                    collaborative manufacturing cells.
                  </div>
                  <br>
              </li>

              <li>
                (Paper ID #16) HRI-DGDM: Dual-Graph Guided Diffusion Model for Uncertain
                Human Motion Modeling in HRI
                <!-- <a class="linkpaper" href="./docs/corl_llhomerobots_2024-cr_paper_1.pdf" target="_blank"> [paper]</a> -->
                <span style="color: #ff8c00">(spotlight)</span>
                <br>
                <span class="authorname">
                Hongquan Gui, Zhanpeng Yang, Xiaoxuan Gan, Ming Li
                </span>
               <br>
                <a data-toggle="collapse" data-target="#abstract16" class="collapsed abstract" aria-expanded="false"> Abstract</a>
                  <div id="abstract16" class="news collapse" style="margin-top: 10px; height: 22px; font-weight: normal;" aria-expanded="false">
                  Human motion in human-robot interaction (HRI)
                  is inherently uncertain, even when performing the same task
                  repeatedly. This variability poses a significant challenge for
                  prediction, as models must capture a distribution of plausible
                  futures rather than a single deterministic trajectory. Traditional
                  graph convolutional network based models, while effective at
                  capturing spatial temporal dependencies, are fundamentally
                  limited by their deterministic nature and struggle to represent
                  this inherent motion uncertainty. To address this, diffusion
                  models have emerged as a powerful framework for modeling
                  uncertainty. However, their direct application to HRI is hindered
                  by two key limitations: they often prioritize motion diversity over
                  prediction accuracy, potentially generating physically
                  implausible results, and they fail to adequately model the
                  complex, multi-scale spatial temporal coupling between human
                  and robot motions. To overcome these challenges, we propose
                  HRI-DGDM, a HRI motion prediction framework based on a
                  dual-graph guided diffusion model. Our method introduces a
                  dual-graph structure—comprising a structural graph for
                  kinematic priors and a collaboration graph learned from motion
                  dynamics—to guide the denoising process with strong structural
                  priors. A dedicated spatial temporal denoising network (STDN)
                  fuses multi-scale features from both graphs through adaptive
                  fusion and hierarchical spatial temporal modeling. Furthermore,
                  a masking-based conditioning mechanism anchors the observed
                  history during denoising, ensuring temporal consistency and
                  preventing drift. Experiments on HRI scenarios demonstrate
                  that HRI-DGDM outperforms baselines in prediction accuracy.
                  </div>
                  <br>
              </li>
              <li>
                (Paper ID #17) External Impulse Perception of Humanoid Robots
                <!-- <a class="linkpaper" href="./docs/corl_llhomerobots_2024-cr_paper_1.pdf" target="_blank"> [paper]</a> -->
                <span style="color: #ff8c00">(spotlight)</span>
                <br>
                <span class="authorname">
                Xingzhou Chen, Yuquan Wang, Ling Shi, Xiayan Xu
                </span>
               <br>
                <a data-toggle="collapse" data-target="#abstract16" class="collapsed abstract" aria-expanded="false"> Abstract</a>
                  <div id="abstract16" class="news collapse" style="margin-top: 10px; height: 22px; font-weight: normal;" aria-expanded="false">
                  Many humanoid robots today are designed without
                  force or torque sensors for simplicity and cost. These sensor-free
                  robots can still acquire locomotion policies through reinforcement
                  learning, as raw motor signals implicitly encode kinematic and
                  dynamic information. However, for more complex tasks such
                  as loco-manipulation and human–robot interaction, the absence
                  of explicit contact awareness poses a significant challenge. To
                  address this, we propose a flow matching-based estimator that
                  decodes external impulses—including contact occurrence, location, magnitude, and direction—from raw joint motor signals and
                  IMU data. Experimental results demonstrate that our method
                  accurately infers external impulses during either standing or
                  walking and outperforms baseline approaches.
                  </div>
                  <br>
              </li> 

            </ul>
          </div>
        </div>
      </div>
    </section>

  <br>
  <hr class="half-rule" />
  <section id="callpapers" style="padding: 40px 0 0 0;">
    <div class="container">
      <div class="row">
        <div class="col-md-10 mx-auto">
          <span class=titlesec>Call for Papers</span><br>

          <h5 style="font-weight: bold"> Submission Guidelines </h5>
          IROS 2025 GenAI4Robotics suggests <b>2+N (short) or 4+N (full) paper length</b> formats — i.e., 2 or 4 pages
          of main content with unlimited additional pages for references, appendices, etc. <br><br>
          Submissions will be handled through: <a
            href="https://docs.google.com/forms/d/e/1FAIpQLSdDPskxt_9enLWGdM-3BHri24igeLFg09A6aJBFNb8oXdmsJQ/viewform?usp=dialog"
            target="_blank">Call for Contributions</a> or email <a href="mailto:genai4robotics@gmail.com">genai4robotics@gmail.com</a>.<br><br>
          We will accept the official <a target="_blank"
            href="https://www.ieee.org/conferences/publishing/templates.html"> IEEE conference</a> paper template.
          <br><br>
          <!-- Our review process will be <b>double-blind</b>, following the CoRL 2024 paper submission policy. -->
          <br><br>
          All accepted papers will be invited for poster presentations; the highest-rated papers, according to the
          Technical Program Committee, will be given spotlight presentations. Accepted papers will be made available
          online on this workshop website as <b>non-archival</b> reports, allowing authors to also submit their works to
          future conferences or journals. We will highlight the Best Reviewer and reveal the Best Paper Award during the
          closing remarks at the workshop event.<br><br>

          Key areas we are targeting in this workshop include:</br>
          </br>
          <ul>
            <li>AI-powered industrial digitalisation and automation</br>
            <li>Human-robot collaboration enhanced by generative AI</br>
            <li>Foundational models in robotics control and manipulation</br>
            <li>VLM/LLM-driven robotic manufacturing/assembly systems</br>
            <li>Data-efficient generative AI for robotics and manufacturing</br>
            <li>Vision-language-conditioned robot navigation and manipulation</br>
            <li>Generative models for robotic learning in manufacturing systems</br>
            <li>Human-in-the-loop reinforcement learning for precision manipulation</br>
            <li>Industrial large models and foundation models in smart manufacturing</br>


          </ul>


          <h5 style="font-weight: bold"> Important Dates </h5>
    <ul>
    <li style="display: list-item">
    <b>Submission deadline:</b> <font color="#000000">1 October, 2025</font>, 23:59 AOE.
    </li>
    <li style="display: list-item">
    <b>Author Notifications:</b> 6 October 2025, 23:59 AOE.
    </li>
    <!-- <li style="display: list-item">
    <s><b>Camera Ready:</b> 3 November 2024, 23:59 AOE.</s>
    </li> -->
    <li style="display: list-item">
    <b>Workshop:</b> 24 October 2025, Full-day
    </li>
    </ul>
        </div>
      </div>
    </div>
    </div>
    </div>
  </section>

  <hr class="half-rule" />
  <section id="organizers">
    <div class="container">
      <div class="row">
        <div class="col-md-10 mx-auto">
          <span class=titlesec>Organizers</span><br>
          <div class="row">
            <a href="#">
              <div class="profpic xlarge-1 columns">
                <img src=./images/organizers/sichao.jpeg class="figure-img img-fluid ">
                <p class=profname>
                  <a href="https://sites.google.com/view/sichao-liu/home"> Sichao Liu </a>
                </p>
                <p class=institution> University of Cambridge & EPFL <a href="mailto:sichao.liu@epfl.ch">sichao.liu@epfl.ch</a> </p>
              </div>
            </a>
            <a href="#">
              <div class="profpic xlarge-1 columns">
                <img src=./images/organizers/chenguang.jpg class="figure-img img-fluid ">
                <p class=profname>
                  <a href="http://www2.informatik.uni-freiburg.de/~huang/">Chenguang Huang</a>
                </p>
                <p class=institution>University of Technology Nuremberg</p>
              </div>
            </a>
            <a href="#">
              <div class="profpic xlarge-1 columns">
                <img src=./images/organizers/hanzhi.jpg class="figure-img img-fluid ">
                <p class=profname>
                  <a href="https://hanzhic.github.io/"> Hanzhi Chen </a>
                </p>
                <p class=institution> Technical University of Munich </p>
              </div>
            </a>
          </div>
          <div class="row">
            <a href="#">
              <div class="profpic xlarge-1 columns">
                <img src=./images/organizers/yuquan.jpeg class="figure-img img-fluid ">
                <p class=profname>
                  <a href="https://ywang-robotics.github.io/">Yuquan Wang</a>
                </p>
                <p class=institution> Tencent Robotics X</p>
              </div>
            </a>
            <a href="#">
              <div class="profpic xlarge-1 columns">
                <img src=./images/organizers/pai.jpg class="figure-img img-fluid ">
                <p class=profname>
                  <a href="https://www.polyu.edu.hk/ise/people/academic-staff/pai-zheng/">Pai Zheng</a>
                </p>
                <p class=institution> The Hong Kong Polytechnic University </p>
              </div>
            </a>
            <a href="#">
              <div class="profpic xlarge-1 columns">
                <img src='./images/organizers/xiaolong.png' class="figure-img img-fluid ">
                <p class="profname">
                  <a href="https://www.linkedin.com/in/dr-xiaolong-feng-263231a/"> Xiaolong Feng </a>
                </p>
                <p class=institution> ABB Corporate Research in Sweden </p>
              </div>
            </a>
            <a href="#">
              <div class="profpic xlarge-1 columns">
                <img src='./images/organizers/wolfram.jpeg' class="figure-img img-fluid ">
                <p class="profname">
                  <a href="https://www.utn.de/person/wolfram-burgard-2/"> Wolfram Burgard </a>
                </p>
                <p class=institution> University of Technology Nuremberg </p>
              </div>
            </a>
          </div>
          </a>
        </div>
      </div>
    </div>
  </section>
  
<br>
<hr class="half-rule" />
<section id="contact-sponsors">
  <div class="container">
    <div class="row">
      <!-- Left: Contact -->
      <div class="col-md-6" style="padding: 20px;">
        <span class="titlesec">Contact and Information</span><br>
        <span style="text-align:left; display:block;">
          Direct questions to <a href="mailto:genai4robotics@gmail.com">genai4robotics@gmail.com</a> or
          <a href="mailto:sichao.liu@epfl.ch">sichao.liu@epfl.ch</a>.
        </span>
        <br>
        <span style="text-align:left; display:block;">
          Subscribe to our <a target="_blank"
            href="https://mailchi.mp/07f8ab4c1c65/llhomerobots-workshop">mailing list</a> to stay updated on news from our workshop series.
        </span>
      </div>

      <!-- Right: Sponsors -->
      <div class="col-md-6 text-center" style="padding: 20px;">
        <span class="titlesec">Sponsors</span><br>
        <a href="https://en.nokov.com/" target="_blank" rel="noopener noreferrer">
        <img src="../images/sponsor/nokov.jpg" style="width: 40%; max-width: 400px; height: auto;" alt="Sponsor Logo">
        </a>
</div>
    </div>
  </div>
</section>
<hr class="half-rule" />

  <!-- Footer -->
  <!-- Bootstrap core JavaScript -->
  <!-- 	<script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js"></script>
      <script src="vendor/jquery/jquery.min.js"></script>
      <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
      <!-- Plugin JavaScript -->
  <!-- <script src="vendor/jquery-easing/jquery.easing.min.js"></script> --> -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.4.1/jquery.easing.min.js
      "> </script>
  <!-- Custom JavaScript for this theme -->
  <script src="js/scrolling-nav.js"></script>
</body>

</html>
